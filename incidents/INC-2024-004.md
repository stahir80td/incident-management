---
incident_id: INC-2024-004
severity: high
service: logging-aggregator
date: 2024-02-10
---

# Disk Space Exhaustion on Logging Server

## Summary
The central logging aggregator ran out of disk space, causing log ingestion failures across all services. Debugging capabilities were severely impacted during the outage.

## Impact
- 4 hours of missing logs across all services
- Unable to troubleshoot concurrent issues
- Log retention reduced from 90 days to 30 days
- 2.3 TB of historical logs purged

## Timeline
- 11:45 UTC: Disk space alerts at 95% utilization
- 12:15 UTC: Disk reached 100%, log ingestion failed
- 12:20 UTC: On-call engineer paged
- 12:40 UTC: Emergency log rotation initiated
- 13:30 UTC: 500 GB freed, ingestion resumed
- 15:45 UTC: Long-term fix deployed with log compression

## Root Cause
Log volume increased 300% over the past 2 weeks due to verbose debug logging left enabled in production after a troubleshooting session. The logging aggregator's disk capacity planning did not account for this spike.

## Resolution
1. Purged older logs to free immediate space
2. Disabled debug logging across all production services
3. Implemented log compression reducing storage by 65%
4. Added automatic log rotation every 7 days
5. Provisioned additional 5 TB storage capacity

## Prevention
- Implement disk space monitoring with 70%, 85%, 95% alerts
- Create automated log level management system
- Require approval for production debug logging with auto-disable after 24h
- Set up predictive disk usage alerts based on growth trends
- Add log volume metrics to service dashboards
- Document log retention policies and enforcement
