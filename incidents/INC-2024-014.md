---
incident_id: INC-2024-014
severity: critical
service: multiple-services
date: 2024-04-23
---

# Cascading Failure from Authentication Service

## Summary
Authentication service slowdown triggered cascading failures across 12 dependent services due to missing timeout configurations and lack of circuit breakers.

## Impact
- 12 services degraded or offline
- Complete platform outage for 18 minutes
- 89% error rate at peak
- $127,000 estimated revenue loss
- 2,800+ support tickets

## Timeline
- 15:30 UTC: Auth service response time increased to 8 seconds
- 15:32 UTC: Dependent services began timing out
- 15:35 UTC: Cascade reached payment and checkout services
- 15:38 UTC: Platform effectively down
- 15:40 UTC: Emergency war room started
- 15:45 UTC: Identified auth service database lock
- 15:55 UTC: Killed blocking query
- 16:08 UTC: Services gradually recovering
- 16:48 UTC: Full platform recovery confirmed

## Root Cause
A long-running database migration locked the auth_users table. The auth service had no query timeout configured and blocked all authentication requests. Dependent services had no timeouts or circuit breakers, causing threads to pile up waiting for auth responses.

## Resolution
1. Terminated blocking database migration
2. Rolled back partial migration
3. Restarted auth service to clear thread pool
4. Dependent services recovered automatically
5. Completed migration during maintenance window

## Prevention
- Implement mandatory timeout configuration (connect: 5s, read: 30s) for all services
- Deploy circuit breakers for all inter-service calls
- Add database migration testing in staging with production-like load
- Implement gradual rollout for database migrations
- Create service dependency mapping and SLA documentation
- Add bulkhead pattern to isolate failure domains
- Require resilience patterns review in architecture reviews
- Conduct chaos engineering tests quarterly
